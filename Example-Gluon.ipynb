{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon import nn, rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(path + 'train.txt')\n",
    "        self.valid = self.tokenize(path + 'valid.txt')\n",
    "        self.test = self.tokenize(path + 'test.txt')\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r') as f:\n",
    "            ids = np.zeros((tokens,), dtype='int32')\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "\n",
    "        return mx.nd.array(ids, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon import nn, rnn\n",
    "\n",
    "class RNNModel(gluon.Block):\n",
    "    \"\"\"A model with an encoder, recurrent layer, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, mode, vocab_size, num_embed, num_hidden,\n",
    "                 num_layers, dropout=0.5, tie_weights=False, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.drop = nn.Dropout(dropout)\n",
    "            self.encoder = nn.Embedding(vocab_size, num_embed,\n",
    "                                        weight_initializer=mx.init.Uniform(0.1))\n",
    "            if mode == 'rnn_relu':\n",
    "                self.rnn = rnn.RNN(num_hidden, 'relu', num_layers, dropout=dropout,\n",
    "                                   input_size=num_embed)\n",
    "            elif mode == 'rnn_tanh':\n",
    "                self.rnn = rnn.RNN(num_hidden, num_layers, dropout=dropout,\n",
    "                                   input_size=num_embed)\n",
    "            elif mode == 'lstm':\n",
    "                self.rnn = rnn.LSTM(num_hidden, num_layers, dropout=dropout,\n",
    "                                    input_size=num_embed)\n",
    "            elif mode == 'gru':\n",
    "                self.rnn = rnn.GRU(num_hidden, num_layers, dropout=dropout,\n",
    "                                   input_size=num_embed)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid mode %s. Options are rnn_relu, \"\n",
    "                                 \"rnn_tanh, lstm, and gru\"%mode)\n",
    "\n",
    "            if tie_weights:\n",
    "                self.decoder = nn.Dense(vocab_size, in_units=num_hidden,\n",
    "                                        params=self.encoder.params)\n",
    "            else:\n",
    "                self.decoder = nn.Dense(vocab_size, in_units=num_hidden)\n",
    "\n",
    "            self.num_hidden = num_hidden\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        emb = self.drop(self.encoder(inputs))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.reshape((-1, self.num_hidden)))\n",
    "        return decoded, hidden\n",
    "\n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 100] loss 7.15, ppl 1272.16\n",
      "[Epoch 0 Batch 200] loss 6.44, ppl 624.02\n",
      "[Epoch 0 Batch 300] loss 6.18, ppl 484.38\n",
      "[Epoch 0 Batch 400] loss 5.94, ppl 378.17\n",
      "[Epoch 0 Batch 500] loss 5.81, ppl 334.23\n",
      "[Epoch 0 Batch 600] loss 5.63, ppl 277.33\n",
      "[Epoch 0 Batch 700] loss 5.60, ppl 270.97\n",
      "[Epoch 0 Batch 800] loss 5.58, ppl 265.91\n",
      "[Epoch 0] time cost 330.34s, valid loss 5.41, valid ppl 223.47\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ParameterDict' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-e901b3c9d581>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mtest_L\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-e901b3c9d581>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mbest_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_L\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mtest_L\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test loss %.2f, test ppl %.2f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_L\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_L\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/mxnet/gluon/parameter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filename, strip_prefix)\u001b[0m\n\u001b[1;32m    500\u001b[0m                         strip_prefix, param.name, strip_prefix))\n\u001b[1;32m    501\u001b[0m             \u001b[0marg_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrip_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     def load(self, filename, ctx, allow_missing=False,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/mxnet/ndarray.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(fname, data)\u001b[0m\n\u001b[1;32m   2367\u001b[0m         raise ValueError(\"data needs to either be a NDArray, dict of str, NDArray pairs \"\n\u001b[1;32m   2368\u001b[0m                          \"or a list of NDarrays.\")\n\u001b[0;32m-> 2369\u001b[0;31m     check_call(_LIB.MXNDArraySave(c_str(fname),\n\u001b[0m\u001b[1;32m   2370\u001b[0m                                   \u001b[0mmx_uint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2371\u001b[0m                                   \u001b[0mc_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNDArrayHandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/mxnet/base.py\u001b[0m in \u001b[0;36mc_str\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mHello\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWorld\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \"\"\"\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ParameterDict' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "\n",
    "\n",
    "args_batch_size = 32\n",
    "args_nhid = 100\n",
    "args_nlayers = 2\n",
    "args_dropout = 0.2\n",
    "args_tied = 'store_true'\n",
    "args_model = 'lstm'\n",
    "args_emsize = 100\n",
    "args_lr = 1.0\n",
    "args_epochs = 2\n",
    "args_bptt = 35\n",
    "args_clip = 0.2\n",
    "args_log_interval = 100\n",
    "\n",
    "###############################################################################\n",
    "# Load data\n",
    "###############################################################################\n",
    "\n",
    "context = mx.cpu(0)\n",
    "\n",
    "corpus = Corpus('./data/ptb.')\n",
    "\n",
    "def batchify(data, batch_size):\n",
    "    \"\"\"Reshape data into (num_example, batch_size)\"\"\"\n",
    "    nbatch = data.shape[0] // batch_size\n",
    "    data = data[:nbatch * batch_size]\n",
    "    data = data.reshape((batch_size, nbatch)).T\n",
    "    return data\n",
    "\n",
    "train_data = batchify(corpus.train, args_batch_size).as_in_context(context)\n",
    "val_data = batchify(corpus.valid, args_batch_size).as_in_context(context)\n",
    "test_data = batchify(corpus.test, args_batch_size).as_in_context(context)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Build the model\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "model = RNNModel(args_model, ntokens, args_emsize, args_nhid,\n",
    "                       args_nlayers, args_dropout, args_tied)\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=context)\n",
    "trainer = gluon.Trainer(model.collect_params(), 'sgd',\n",
    "                        {'learning_rate': args_lr,\n",
    "                         'momentum': 0,\n",
    "                         'wd': 0})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "###############################################################################\n",
    "# Training code\n",
    "###############################################################################\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(args_bptt, source.shape[0] - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len]\n",
    "    return data, target.reshape((-1,))\n",
    "\n",
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [i.detach() for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden\n",
    "\n",
    "def eval(data_source):\n",
    "    total_L = 0.0\n",
    "    ntotal = 0\n",
    "    hidden = model.begin_state(func=mx.nd.zeros, batch_size=args_batch_size, ctx=context)\n",
    "    for i in range(0, data_source.shape[0] - 1, args_bptt):\n",
    "        data, target = get_batch(data_source, i)\n",
    "        output, hidden = model(data, hidden)\n",
    "        L = loss(output, target)\n",
    "        total_L += mx.nd.sum(L).asscalar()\n",
    "        ntotal += L.size\n",
    "    return total_L / ntotal\n",
    "\n",
    "def train():\n",
    "    best_val = float(\"Inf\")\n",
    "    for epoch in range(args_epochs):\n",
    "        total_L = 0.0\n",
    "        start_time = time.time()\n",
    "        hidden = model.begin_state(func=mx.nd.zeros, batch_size=args_batch_size, ctx=context)\n",
    "        for ibatch, i in enumerate(range(0, train_data.shape[0] - 1, args_bptt)):\n",
    "            data, target = get_batch(train_data, i)\n",
    "            hidden = detach(hidden)\n",
    "            with autograd.record():\n",
    "                output, hidden = model(data, hidden)\n",
    "                L = loss(output, target)\n",
    "                L.backward()\n",
    "\n",
    "            grads = [i.grad(context) for i in model.collect_params().values()]\n",
    "            # Here gradient is for the whole batch.\n",
    "            # So we multiply max_norm by batch_size and bptt size to balance it.\n",
    "            gluon.utils.clip_global_norm(grads, args_clip * args_bptt * args_batch_size)\n",
    "\n",
    "            trainer.step(args_batch_size)\n",
    "            total_L += mx.nd.sum(L).asscalar()\n",
    "\n",
    "            if ibatch % args_log_interval == 0 and ibatch > 0:\n",
    "                cur_L = total_L / args_bptt / args_batch_size / args_log_interval\n",
    "                print('[Epoch %d Batch %d] loss %.2f, ppl %.2f'%(\n",
    "                    epoch, ibatch, cur_L, math.exp(cur_L)))\n",
    "                total_L = 0.0\n",
    "\n",
    "        val_L = eval(val_data)\n",
    "\n",
    "        print('[Epoch %d] time cost %.2fs, valid loss %.2f, valid ppl %.2f'%(\n",
    "            epoch, time.time()-start_time, val_L, math.exp(val_L)))\n",
    "\n",
    "        if val_L < best_val:\n",
    "            best_val = val_L\n",
    "            test_L = eval(test_data)\n",
    "            model.collect_params().save(model.params)\n",
    "            print('test loss %.2f, test ppl %.2f'%(test_L, math.exp(test_L)))\n",
    "        else:\n",
    "            args_lr = args_lr*0.25\n",
    "            trainer._init_optimizer('sgd',\n",
    "                                    {'learning_rate': args_lr,\n",
    "                                     'momentum': 0,\n",
    "                                     'wd': 0})\n",
    "            model.collect_params().load(model.params, context)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n",
    "    model.collect_params().load(model.params, context)\n",
    "    test_L = eval(test_data)\n",
    "    print('Best test loss %.2f, test ppl %.2f'%(test_L, math.exp(test_L)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
