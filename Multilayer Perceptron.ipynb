{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./data/train_data_50000.json\") as json_data:\n",
    "    train_data = json.load(fp=json_data)\n",
    "    \n",
    "with open(\"./data/test_data_50000.json\") as json_data:\n",
    "    test_data = json.load(fp=json_data)\n",
    "\n",
    "train_X = np.array(train_data['data'])\n",
    "train_Y = np.array(train_data['label'])\n",
    "\n",
    "test_X = np.array(test_data['data'])\n",
    "test_Y = np.array(test_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "num_inputs = 64\n",
    "num_outputs = 2\n",
    "train_data = mx.io.NDArrayIter(train_X,train_Y ,\n",
    "                               batch_size, shuffle=True)\n",
    "test_data = mx.io.NDArrayIter(test_X, test_Y,\n",
    "                               batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_hidden = 256\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    data_iterator.reset()\n",
    "    for i, batch in enumerate(data_iterator):\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.677382715461, Train_acc 0.5994, Test_acc 0.13634\n",
      "Epoch 1. Loss: 0.66288332207, Train_acc 0.60766, Test_acc 0.11466\n",
      "Epoch 2. Loss: 0.658119773957, Train_acc 0.61322, Test_acc 0.10722\n",
      "Epoch 3. Loss: 0.654353553126, Train_acc 0.61816, Test_acc 0.10558\n",
      "Epoch 4. Loss: 0.65041174969, Train_acc 0.6234, Test_acc 0.10602\n",
      "Epoch 5. Loss: 0.646193166805, Train_acc 0.62984, Test_acc 0.10818\n",
      "Epoch 6. Loss: 0.641675029941, Train_acc 0.63526, Test_acc 0.1112\n",
      "Epoch 7. Loss: 0.63696968901, Train_acc 0.64174, Test_acc 0.11702\n",
      "Epoch 8. Loss: 0.632123405727, Train_acc 0.64764, Test_acc 0.13026\n",
      "Epoch 9. Loss: 0.627179627871, Train_acc 0.65474, Test_acc 0.14478\n",
      "Epoch 10. Loss: 0.622283751739, Train_acc 0.65976, Test_acc 0.1608\n",
      "Epoch 11. Loss: 0.617577099847, Train_acc 0.66644, Test_acc 0.17588\n",
      "Epoch 12. Loss: 0.613101373794, Train_acc 0.67242, Test_acc 0.19374\n",
      "Epoch 13. Loss: 0.608835831395, Train_acc 0.67808, Test_acc 0.21464\n",
      "Epoch 14. Loss: 0.604724415204, Train_acc 0.68194, Test_acc 0.23328\n",
      "Epoch 15. Loss: 0.600911915898, Train_acc 0.68676, Test_acc 0.24564\n",
      "Epoch 16. Loss: 0.59723077499, Train_acc 0.69028, Test_acc 0.25724\n",
      "Epoch 17. Loss: 0.593553681346, Train_acc 0.69382, Test_acc 0.2654\n",
      "Epoch 18. Loss: 0.589955257582, Train_acc 0.6973, Test_acc 0.26986\n",
      "Epoch 19. Loss: 0.586504278757, Train_acc 0.69954, Test_acc 0.27746\n",
      "Epoch 20. Loss: 0.583001859487, Train_acc 0.70294, Test_acc 0.2852\n",
      "Epoch 21. Loss: 0.579532900382, Train_acc 0.70584, Test_acc 0.28784\n",
      "Epoch 22. Loss: 0.576247418411, Train_acc 0.70818, Test_acc 0.29252\n",
      "Epoch 23. Loss: 0.572915929944, Train_acc 0.71236, Test_acc 0.29298\n",
      "Epoch 24. Loss: 0.569513532105, Train_acc 0.71558, Test_acc 0.29694\n",
      "Epoch 25. Loss: 0.566251347672, Train_acc 0.7187, Test_acc 0.30318\n",
      "Epoch 26. Loss: 0.562909571594, Train_acc 0.7214, Test_acc 0.30518\n",
      "Epoch 27. Loss: 0.559601174978, Train_acc 0.72378, Test_acc 0.30168\n",
      "Epoch 28. Loss: 0.556650355755, Train_acc 0.72572, Test_acc 0.30246\n",
      "Epoch 29. Loss: 0.553526981103, Train_acc 0.72722, Test_acc 0.30268\n",
      "Epoch 30. Loss: 0.55004485719, Train_acc 0.7301, Test_acc 0.30658\n",
      "Epoch 31. Loss: 0.546233880128, Train_acc 0.7322, Test_acc 0.3078\n",
      "Epoch 32. Loss: 0.54291996873, Train_acc 0.73474, Test_acc 0.30536\n",
      "Epoch 33. Loss: 0.539525534997, Train_acc 0.7373, Test_acc 0.30738\n",
      "Epoch 34. Loss: 0.535399585803, Train_acc 0.74134, Test_acc 0.31378\n",
      "Epoch 35. Loss: 0.531477702771, Train_acc 0.7427, Test_acc 0.30964\n",
      "Epoch 36. Loss: 0.52744718532, Train_acc 0.7467, Test_acc 0.3223\n",
      "Epoch 37. Loss: 0.523910068375, Train_acc 0.75286, Test_acc 0.34838\n",
      "Epoch 38. Loss: 0.520192751023, Train_acc 0.7496, Test_acc 0.3134\n",
      "Epoch 39. Loss: 0.51614079105, Train_acc 0.75616, Test_acc 0.32824\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "moving_loss = 0.\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_data.reset()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            cross_entropy = loss(output, label)\n",
    "            cross_entropy.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        if i == 0:\n",
    "            moving_loss = nd.mean(cross_entropy).asscalar()\n",
    "        else:\n",
    "            moving_loss = .99 * moving_loss + .01 * nd.mean(cross_entropy).asscalar()\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, moving_loss, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
